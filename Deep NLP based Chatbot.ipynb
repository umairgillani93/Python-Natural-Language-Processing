{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a seq2seq model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining model's inputs\n",
    "\n",
    "def model_inputs():\n",
    "    inputs = tf.placeholder(tf.int32, [None,None], name = 'input')\n",
    "    targets = tf.placeholder(tf.int32, [None,None], name = 'target')\n",
    "    lr = tf.placeholder(tf.float32, name = 'learning_rate')\n",
    "    keep_prob = tf.placehodler(tf.float32, name = 'keep_prob') # neurons dropout proportion\n",
    "    \n",
    "    return inputs, targets, lr, keep_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processed_targets(targets, word2int, batch_size):\n",
    "    \"\"\"\n",
    "    - processes the targets (actual answers) in such a way that decoder understands them\n",
    "    - each answer starts off with a special token <SOS>\n",
    "    - answers much be fed to Neural Network in batches. And we'll declare the batch size\n",
    "    - word2int: dictionary that maps words to integers, here tokens to integers!\n",
    "    - targets: actual answers to be processed\n",
    "    - batch_size: number of answers in a batch\n",
    "    - We'll convert first index of the answer into vector embedding containing integer for <SOS>\\\n",
    "    - and we'll chop off the last column, and then concatenate both the first vector column and embedding matrix except the last column\n",
    "    \n",
    "    \"\"\"\n",
    "    left_side = tf.fill([batch_size, 1], word2int['<SOS>'])\n",
    "    right_side = tf.strided_slice(targets, [0,0], [batch_size, 1, [1,1]])\n",
    "    preprocesses_targets = tf.concat([left_side, right_side], 1)\n",
    "    \n",
    "    return preprocessed_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_rnn_layer(rnn_inputs, rnn_size, num_layers, keep_prob, sequence_length):\n",
    "    \"\"\"\n",
    "    rnn inputs: RNN inputs defined above, i.e inputs, targets, lr, keep_prob\n",
    "    rnn_size: size of the the Tensors\n",
    "    num_layes: num of hidden layers\n",
    "    keep_prob: regularization constant, Neurons dropout proportion\n",
    "    sequence_length: list of length of each question in the batch\n",
    "    \n",
    "    stacked LSTM: Multiple hidden LSTM layers, and each layer contains multiple memory cells\n",
    "    \"\"\"\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size) # create a lstm cell with size equals to rnn_size\n",
    "    lstm_dropout = tf.contrib.rnn.DropoutWrapper(lstm, input_keep_prob = keep_prob) # apply dropout on lstm, with dropout proportion = keep_prob\n",
    "    encoder_cell = tf.contrib.rnn.MultiRNNCell([lstm_dropout] * num_layers) # encoder_cell is going to be equals to the lstm_dropout cell times the number of layers we plan to have!\n",
    "    _, encoder_state = tf.nn.bidirectional_dynamic_rnn(cell_fw = encoder_cell,\n",
    "                                                       cell_bw = encoder_cell,\n",
    "                                                        sequence_length = sequence_length,\n",
    "                                                        inputs = rnn_inputs,\n",
    "                                                        dtype = tf.float32)\n",
    "    \n",
    "    return encoder_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder RNN Layer\n",
    "# 1-Decoder training data\n",
    "# 2-Decoder Cross validation set\n",
    "# 3-Decoder RNN layer\n",
    "\n",
    "# Here in the fucntion we'll make use of Tensorflow embeddings. An Embedding is basically a vector representation of text.\n",
    "# The text is first converted into Integer id's, and then applying embedding method from tensorflow it reports back the context\n",
    "# vector of the text id's passed, which id mapped over vector space and maps every single word in vector space!\n",
    "\n",
    "def decode_training_set(encoder_state, decoder_cell, decoder_embedded_input, sequence_length, decoding_scope, output_function,\n",
    "                      keep_prob, batch_size):\n",
    "    \"\"\"\n",
    "    encoder_state: state of the encoder to be pass in as input to decoder\n",
    "    decoder_cell: cell of the decoder\n",
    "    decoder_embedded_input: input of the decoder that is mapped from words to vectors in vector space\n",
    "    sequence_lenght: length of the list of passed words\n",
    "    decoding_scope: to create a tensorflow variable from\n",
    "    output_function: to report back the output of decoder\n",
    "    keep_prob: neurons dropout ratio\n",
    "    batch_size: size of the batch to be passed to decoder!\n",
    "    \"\"\"\n",
    "    \n",
    "    attention_state = tf.zeros([batch_size, 1, decoder_cell.output]) # initializes the attention_states\n",
    "    # batch_size -> number of rows\n",
    "    # 1 -> number of columns\n",
    "    # decoder_cell.output -> number of elements\n",
    "    \n",
    "    attention_keys, attention_values, attention_score_function, attention_construct_function = tf.contrib.seq2seq.prepare_attention(attention_state, attention_option = 'bahdanau', num_units = decoder_cell.output)\n",
    "    # attention_keys: keys to be compared with the target state\n",
    "    # attention_values: values that we'll used to create the context vector\n",
    "    # attention_score_function: used to compute the similarities between the Keys and the Target states\n",
    "    # attention_construct_function: used to build the attention state\n",
    "    \n",
    "    training_decoder_function = tf.contrib.seq2seq.attention_decoder_fn_train(encoder_state[0],\n",
    "                                                                             attention_keys,\n",
    "                                                                             attention_values,\n",
    "                                                                             attention_score_function,\n",
    "                                                                             attention_construct_function,\n",
    "                                                                             name = 'attn_dec_train')\n",
    "    \n",
    "    decoder_output, decoder_final_state, decoder_final_context_state = tf.contrib.seq2seq.dynamic_rnn_decoder(decoder_cell,\n",
    "                                                                                                             training_decoder_function,\n",
    "                                                                                                             decoder_embedded_input,\n",
    "                                                                                                             sequence_length,\n",
    "                                                                                                             scope = decoding_scope)\n",
    "    \n",
    "    decoder_output_dropout = tf.nn.dropout(decoder_output, keep_prob)\n",
    "    # decoder_output: object on which dropout has to be applied\n",
    "    # keep_prob: ration of dropout be applied on decoder_output\n",
    "    \n",
    "    return output_function(decoder_output_dropout)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_test_set(encoder_state, decoder_cell, decoder_embedding_matrix, sos_id, eos_id, maximum_length, num_words,\n",
    "                   decoding_scope, output_function, keep_prob, batch_size):\n",
    "    \"\"\"\n",
    "    sos_id: id of the start of sentence token\n",
    "    eos_id: id of the end of the senetence token\n",
    "    maximum_length: maximum length of the answer\n",
    "    num_words: total number of words of all the answers. For this we have to take answers_words_2_int dictionary\n",
    "    And we are using these extra arguments because of the 'infer' method we are gonna use in this current funciton\n",
    "    rest of the things are same as above method!\n",
    "    \"\"\"\n",
    "    \n",
    "    attention_state = tf.zeros([batch_size, 1, decoder_cell.output]) # initializes the attention_states\n",
    "    # batch_size -> number of rows\n",
    "    # 1 -> number of columns\n",
    "    # decoder_cell.output -> number of elements\n",
    "    \n",
    "    attention_keys, attention_values, attention_score_function, attention_construct_function = tf.contrib.seq2seq.prepare_attention(attention_state, attention_option = 'bahdanau', num_units = decoder_cell.output)\n",
    "    # attention_keys: keys to be compared with the target state\n",
    "    # attention_values: values that we'll used to create the context vector\n",
    "    # attention_score_function: used to compute the similarities between the Keys and the Target states\n",
    "    # attention_construct_function: used to build the attention state\n",
    "    \n",
    "    test_decoder_function = tf.contrib.seq2seq.attention_decoder_fn_inference(output_function,\n",
    "                                                                             encoder_state[0],\n",
    "                                                                             attention_keys,\n",
    "                                                                             attention_values,\n",
    "                                                                             attention_score_function,\n",
    "                                                                             attention_construct_function,\n",
    "                                                                             decoder_embedding_matrix,\n",
    "                                                                             sos_id,\n",
    "                                                                             eos_id, maximum_length, num_words, \n",
    "                                                                             name = 'attn_dec_inf')\n",
    "    \n",
    "    test_predictions, decoder_final_state, decoder_final_context_state = tf.contrib.seq2seq.dynamic_rnn_decoder(decoder_cell,\n",
    "                                                                                                             test_decoder_function,\n",
    "                                                                                                             decoder_embedded_input,\n",
    "                                                                                                             scope = decoding_scope)\n",
    "    \n",
    "    return test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building RNN decoder\n",
    "\n",
    "def decoder_rnn_layer(decoder_embedded_input, decoder_embedding_matrix, encoder_state, num_words, sequence_length,\n",
    "                     rnn_size, num_layers, word2int, keep_prob, batch_size):\n",
    "    \"\"\"\n",
    "    decoder_embedded_input: input in the formate that decoder receives\n",
    "    decoder_embedding_matrix: decoded embedding matrix for decoder\n",
    "    encoder_state: last hidden state of encoder that goes into decoder\n",
    "    rnn_size: size of the the Tensors\n",
    "    num_layes: num of hidden layers\n",
    "    num_words: total number of words in corpus of answers\n",
    "    keep_prob: regularization constant, Neurons dropout proportion\n",
    "    sequence_length: list of length of each question in the batch\n",
    "    batch_size: size of the batch\n",
    "    word2int: integer ids for each word\n",
    "    \n",
    "    \"\"\"\n",
    "    with tf.variable_scope(\"decoding\") as decoding_scope:\n",
    "        lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "        lstm_dropout = tf.contrib.rnn.DropourtWrapper(lstm, input_keep_prob = keep_prob) # applied dropout on lstm, with percentage of keep_prob\n",
    "        decoder_cell = tf.contrib.rnn.MultiRNNCell([lstm_dropout] * num_layers)\n",
    "        weights = tf.truncated_normal_initializer(stddev = 0.1) # initizlizing the weights\n",
    "        biases = tf.zeros.initializer() # initializing the biases\n",
    "        output_function = lambda x: tf.contrib.layers.fully_connected(x,\n",
    "                                                                     num_words,\n",
    "                                                                     None,\n",
    "                                                                     scope = decoding_scope,\n",
    "                                                                     weights_initializers = weights,\n",
    "                                                                     biases_initializer = biases,)\n",
    "        training_predictions = decode_training_set(encoder_state,\n",
    "                                                  decoder_cell,\n",
    "                                                  decoder_embedded_input,\n",
    "                                                  sequence_length,\n",
    "                                                  decoding_scope,\n",
    "                                                  output_function,\n",
    "                                                  keep_prob,\n",
    "                                                  batch_size)\n",
    "        \n",
    "        decoding_scope.reuse_variables()\n",
    "        \n",
    "        \n",
    "\n",
    "        test_predictions = decode_test_set(encoder_state, decoder_cell,\n",
    "                                                  decoder_embedding_matrix,\n",
    "                                                  word2int['<SOS>'],\n",
    "                                                  word2int['<EOS>'],\n",
    "                                                  sequence_length -1,\n",
    "                                                  num_words,\n",
    "                                                  decoding_scope,\n",
    "                                                  output_function,\n",
    "                                                  keep_prob,\n",
    "                                                  batch_size)\n",
    "        \n",
    "\n",
    "    return training_predictions, test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEQ2SEQ MODEL!\n",
    "\n",
    "def seq2seq_model(inputs, targets, keep_prob, batch_size, sequence_length, answers_num_words, questions_num_words,\n",
    "                 encoder_embedding_size):\n",
    "    \n",
    "    \"\"\"\n",
    "    inputs: questions to chatbot\n",
    "    targets: actual answers of the questions\n",
    "    keep_prob: dropout ratio\n",
    "    batch_size: size of the batch\n",
    "    sequence_length: length of the sequence\n",
    "    answers_num_words: num of words in all the answers\n",
    "    questions_num_words: num of words in all the questions\n",
    "    encoder_embedding_size: num of dimensions of the embedding matrix for the encoder\n",
    "    decoder_embedding_size: num of dimensions of the embedding matrix for the decoder\n",
    "    rnn_size: num of input tensors\n",
    "    num_layers: num of layers with dropout applied\n",
    "    questionswords2int: dictionary that converst questions words into their integer ids\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Lets put all together now!\n",
    "    # firstly we need encoder embedded inputs\n",
    "    encoder_embedded_inputs = tf.contrib.layers.embed-sequence(inputs, answers_num_words + 1, encoder_embedding_size, \n",
    "                                                              initializer = tf.random_uniform_initializer(0,1))\n",
    "    \n",
    "    # inputs: inputs we actually want to embed\n",
    "    # total number of words of the answers\n",
    "    # encoder_embedding_size: num of dimensions in the embedding matrix for encoder\n",
    "    # initializer for our sequence to sequence model\n",
    "    \n",
    "    encoder_state = encoder_rnn_layer(encoder_embedded_input, rnn_size, num_layers, keep_prob, sequence_length)\n",
    "    preprocessed_targets = preprocess_targets(targets, questionswords2int, batch_size)\n",
    "    decoder_embeddings_matrix = tf.Variable(tf.random_uniform([questions_num_words + 1, decoder_embedding_size], 0, 1))\n",
    "    # here questions_num_words and decoder_embedding_size are the size of dimesions, for initializing the decoder_embeddings_matrix\n",
    "    # 0, 1 -> lower bounds and upper bounds of the random initializer\n",
    "    \n",
    "    decoder_embedded_input = tf.nn.embedding_lookup(decoder_embeddings_matrix, preprocessed_targets)\n",
    "    training_predictions, test_predictions = decoder_rnn_layer(decoder_embedded_input, decoder_embeddings_matrix,\n",
    "                                                              encoder_state,\n",
    "                                                              questions_num_words,\n",
    "                                                              sequence_length,\n",
    "                                                              rnn_size,\n",
    "                                                              num_layers,\n",
    "                                                              questionsword2int,\n",
    "                                                              keep_prob,\n",
    "                                                              batch_size)\n",
    "    \n",
    "    return training_predictions, test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
