{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmaztization:\n",
    "\n",
    "Often when searching text for a certain keyword, it helps when the search returns variations of the word!\n",
    "Also lemmatization is much more informative and makes sense than stemming. It looks at the surrouding words and extract the Parts of Speech, it doesn't categorize phrases.\n",
    "\n",
    "For instance, searching for a boat might also return \"boats\" and \"boating\". Here, \"boat\" would be the stem for {boating, boats, boater, boats}\n",
    "\n",
    "Stemming is somewhat crud method for cataloging related words; it essentially chops off the letter from the words untill the stem is reached!\n",
    "\n",
    "This words faily well in most of the cases but, unfortunately English has many exceptions where a more sophisticated process is required.\n",
    "\n",
    "Spacy doesn't include Stemmer, rather it's more lien towards lemmatization. And because Spacy doesn't include stemmers, let's jump onto the NLTK libray in order to understand the stemming process!\n",
    "\n",
    "We'll discuss both Porter Stemmer and Snowball Stemmer, both are the best stemming algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Porter Stemmer:\n",
    "\n",
    "The Algorithm employes different phases of word reduction, each with it's own set of mapping rules. Here's a couple of them:\n",
    "\n",
    "* Initially, simple suffix rules are defined, which correlates to dropping the suffix until a stem word is extracted e.g SSES --> SS or may be Cats --> Cat\n",
    "* More sohpisticated phases consider the length / complexity of the word before applying a rule, e.g (m > 0) --> \"ATIONAL\" = ATE or (m > 0) --> \"EED\" = \"EE\" / relational = relate and agreed = agree\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Snowball Stemmer:\n",
    "\n",
    "Snowball is the name of stemming language, developed by Martin Porter. This Algorithm offers a slight improvement over the Porter Stemmer Algorithm. Let's go ahead and see how to use these Stemmers in Python and NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk # import Natural language toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer # importing the Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_stemmer = PorterStemmer() # instantiating the PorterStemmer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run -----> run\n",
      "ran -----> ran\n",
      "running -----> run\n",
      "easily -----> easili\n",
      "fairly -----> fairli\n",
      "nicely -----> nice\n",
      "awesomely -----> awesom\n",
      "fairness -----> fair\n",
      "far -----> far\n"
     ]
    }
   ],
   "source": [
    "words = ['run', 'ran', 'running', 'easily', 'fairly', 'nicely', 'awesomely', 'fairness', 'far']\n",
    "\n",
    "for word in words:\n",
    "    print(word + ' -----> ' + (p_stemmer.stem(word)))\n",
    "    \n",
    "# As we can see the adverbs e.g \"easily\" have been transformed to some weird words, which end of 'li'\n",
    "# according to the Porter Stemmer Algorithm rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets go ahead and stem them now with Snowball Stemmer\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_stemmer = SnowballStemmer(language = 'english') # we need to provide the language we are using in case of Snowball Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['run',\n",
       " 'ran',\n",
       " 'running',\n",
       " 'easily',\n",
       " 'fairly',\n",
       " 'nicely',\n",
       " 'awesomely',\n",
       " 'fairness',\n",
       " 'far']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run -----> run\n",
      "ran -----> ran\n",
      "running -----> run\n",
      "easily -----> easili\n",
      "fairly -----> fair\n",
      "nicely -----> nice\n",
      "awesomely -----> awesom\n",
      "fairness -----> fair\n",
      "far -----> far\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word + \" -----> \" + s_stemmer.stem(word))\n",
    "    \n",
    "# notice how it extracts the perfect stems for both 'fairness' and 'fairly' i.e 'fair', that's a slight imporvement\n",
    "# over Porter Stemmer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PORTER STEMMER!\n",
      "\n",
      "\n",
      "generous -----> gener\n",
      "generously -----> gener\n",
      "generation -----> gener\n",
      "generate -----> gener\n",
      "\n",
      "\n",
      "===================================\n",
      "\n",
      "\n",
      "SNOWBALL STEMMER!\n",
      "\n",
      "\n",
      "generous ------> generous\n",
      "generously ------> generous\n",
      "generation ------> generat\n",
      "generate ------> generat\n"
     ]
    }
   ],
   "source": [
    "# Let's take another example:\n",
    "\n",
    "words1 = ['generous', 'generously', 'generation', 'generate']\n",
    "\n",
    "print('PORTER STEMMER!')\n",
    "print('\\n')\n",
    "for word in words1:\n",
    "    print(word + ' -----> ' + p_stemmer.stem(word))\n",
    "    \n",
    "print('\\n')\n",
    "print('===================================')\n",
    "print('\\n')\n",
    "print('SNOWBALL STEMMER!')\n",
    "print('\\n')\n",
    "for word in words1:\n",
    "    print(word + ' ------> ' + s_stemmer.stem(word))\n",
    "\n",
    "# Now we can clearly visualize the differenece both the Algorithms have, on the basis of the way they relate\n",
    "# back to a particular stem!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization with Spacy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I \t PRON \t 561228191312463089 \t -PRON-\n",
      "am \t VERB \t 10382539506755952630 \t be\n",
      "a \t DET \t 11901859001352538922 \t a\n",
      "runner \t NOUN \t 12640964157389618806 \t runner\n",
      ", \t PUNCT \t 2593208677638477497 \t ,\n",
      "running \t VERB \t 12767647472892411841 \t run\n",
      "in \t ADP \t 3002984154512732771 \t in\n",
      "a \t DET \t 11901859001352538922 \t a\n",
      "race \t NOUN \t 8048469955494714898 \t race\n",
      "because \t ADP \t 16950148841647037698 \t because\n",
      "I \t PRON \t 561228191312463089 \t -PRON-\n",
      "love \t VERB \t 3702023516439754181 \t love\n",
      "to \t PART \t 3791531372978436496 \t to\n",
      "run \t VERB \t 12767647472892411841 \t run\n",
      "since \t ADP \t 10066841407251338481 \t since\n",
      "I \t PRON \t 561228191312463089 \t -PRON-\n",
      "ran \t VERB \t 12767647472892411841 \t run\n",
      "today \t NOUN \t 11042482332948150395 \t today\n",
      "! \t PUNCT \t 17494803046312582752 \t !\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "doc = nlp(u\"I am a runner, running in a race because I love to run since I ran today!\")\n",
    "\n",
    "# This sentence has got lots of words that have kind of similar meanings. e.g running, run, ran etc\n",
    "# Now let's see how sapCy breaks them down the particular lemmas\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, '\\t', token.pos_, '\\t', token.lemma, '\\t', token.lemma_)\n",
    "    \n",
    "# token.text -> text of the token\n",
    "# token.pos_ -> POS Tag of the word\n",
    "# token.lemma -> hashed number, generated corresponding to particular lemma in dic (en_core_web_sm)\n",
    "# token.lemma_ -> actual lemma for the word!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_lemmas(text):\n",
    "    for token in text:\n",
    "        print(f\"{token.text:{12}} {token.pos_:{6}} {token.lemma:<{22}} {token.lemma_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I            PRON   561228191312463089     -PRON-\n",
      "am           VERB   10382539506755952630   be\n",
      "a            DET    11901859001352538922   a\n",
      "runner       NOUN   12640964157389618806   runner\n",
      ",            PUNCT  2593208677638477497    ,\n",
      "running      VERB   12767647472892411841   run\n",
      "in           ADP    3002984154512732771    in\n",
      "a            DET    11901859001352538922   a\n",
      "race         NOUN   8048469955494714898    race\n",
      "because      ADP    16950148841647037698   because\n",
      "I            PRON   561228191312463089     -PRON-\n",
      "love         VERB   3702023516439754181    love\n",
      "to           PART   3791531372978436496    to\n",
      "run          VERB   12767647472892411841   run\n",
      "since        ADP    10066841407251338481   since\n",
      "I            PRON   561228191312463089     -PRON-\n",
      "ran          VERB   12767647472892411841   run\n",
      "today        NOUN   11042482332948150395   today\n",
      "!            PUNCT  17494803046312582752   !\n"
     ]
    }
   ],
   "source": [
    "show_lemmas(doc) # prints in nice readable form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Words:\n",
    "\n",
    "Words like \"a\" and \"the\" appear so frequently in context, that we don't even want to Tag them out as Nouns, Verbs, and other POS. We call these \"Stop words\" and they need to be filtered out from the text to be processed.\n",
    "\n",
    "Spacy holds a built-in list of some 305 Stop words for English. Let's go ahead and figure out how it works in Spacy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'out', 'fifteen', 'these', 'hereafter', 'some', 'though', 'because', 'make', 'elsewhere', 'anyone', 'am', 'further', 'around', 'itself', 'perhaps', 'does', 'somehow', 'a', 'else', 'please', 'per', 'any', 'hers', 'more', 'then', 'was', 'whereas', 'three', 'yourselves', 'did', 'too', 'can', 'at', 'seeming', 'so', 'former', 'formerly', 'that', 'via', 'of', 'here', 'had', 'another', 'full', 'without', 'your', 'whereafter', 'last', 'their', 'the', 'being', 'cannot', 'four', 'very', 'yourself', 'be', 'latter', 'fifty', 'seems', 'could', 'with', 'themselves', 'again', 'whom', 'only', 'me', 'sometimes', 'this', 'ca', 'his', 'becoming', 'all', 'by', 'may', 'herein', 'besides', 'many', 'no', 'are', 'everywhere', 'became', 'anyhow', 'as', 'been', 'my', 'several', 'amongst', 'next', 'whenever', 'where', 'less', 'who', 'always', 'wherein', 'call', 'two', 'would', 'for', 'hence', 'seem', 'thereupon', 'when', 'top', 'while', 'even', 'it', 'made', 'every', 'before', 'enough', 'someone', 'five', 'she', 'during', 'afterwards', 'ourselves', 'seemed', 'namely', 'once', 'six', 'how', 'those', 'its', 'myself', 'thence', 'either', 'do', 'various', 'have', 'thereafter', 'herself', 'therein', 'say', 'might', 'therefore', 'between', 'why', 'both', 'whereby', 'we', 'what', 'give', 'nobody', 'move', 'nothing', 'he', 'through', 'serious', 'become', 'onto', 'sixty', 'most', 'up', 'will', 'beforehand', 'name', 'whoever', 'upon', 'thru', 'within', 'yours', 'anything', 'never', 'thus', 'ours', 'neither', 'whether', 'since', 'just', 'one', 'see', 'in', 'but', 'empty', 'everything', 'meanwhile', 'than', 'whence', 'sometime', 'few', 'also', 'eleven', 'into', 'nowhere', 'used', 'side', 'first', 'using', 'beyond', 'now', 'hereby', 'him', 'anywhere', 'from', 'quite', 'across', 'part', 'about', 'becomes', 'otherwise', 'amount', 'indeed', 'really', 'own', 'ten', 'off', 'twenty', 'whole', 're', 'whose', 'such', 'after', 'eight', 'due', 'everyone', 'they', 'none', 'over', 'third', 'below', 'others', 'toward', 'under', 'show', 'us', 'done', 'already', 'which', 'doing', 'back', 'whatever', 'anyway', 'nine', 'against', 'our', 'forty', 'still', 'them', 'whereupon', 'there', 'hundred', 'among', 'behind', 'same', 'unless', 'much', 'together', 'noone', 'were', 'keep', 'you', 'is', 'something', 'has', 'mine', 'other', 'rather', 'go', 'should', 'get', 'and', 'on', 'although', 'often', 'almost', 'to', 'if', 'her', 'i', 'down', 'beside', 'each', 'regarding', 'along', 'thereby', 'mostly', 'alone', 'ever', 'or', 'throughout', 'above', 'an', 'nor', 'hereupon', 'not', 'somewhere', 'well', 'latterly', 'whither', 'bottom', 'yet', 'front', 'except', 'moreover', 'himself', 'towards', 'however', 'until', 'wherever', 'twelve', 'must', 'least', 'take', 'put', 'nevertheless'}\n"
     ]
    }
   ],
   "source": [
    "print(nlp.Defaults.stop_words) # throws back the set of all the stop words spacy has"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab['is'].is_stop # reports back 'is' is a stop word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab['mystery'].is_stop # reports back 'mystery' is not a stop word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.Defaults.stop_words.add('btw') # adds 'btw' as a stop word\n",
    "nlp.vocab['btw'].is_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "306"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nlp.Defaults.stop_words) # reports back 'btw' is added to the set of stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab['btw'].is_stop # tell it's a stop word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now go ahead and remove some of the stop words from the built-in spacy set\n",
    "\n",
    "nlp.Defaults.stop_words.remove('few')\n",
    "\n",
    "nlp.vocab['few'].is_stop = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab['few'].is_stop # successfully removed from the list of stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Good Job!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
