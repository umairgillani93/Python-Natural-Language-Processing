{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Segmentation:\n",
    "\n",
    "Now let's go ahead and see how we can divide sentences into segments. This really helps when working with NLP algorithms in order to find out the contextual meanings of a particular sentence.\n",
    "\n",
    "Let's jump into its implementation.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm') # loading vocabulary for English language!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(u\"This is the first sentence. This is another sentence. This is the last sentence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the first sentence.\n",
      "This is another sentence.\n",
      "This is the last sentence\n"
     ]
    }
   ],
   "source": [
    "for sent in doc.sents:\n",
    "    print(sent) # prints every single sentence inside our doc object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep in mind the doc.sents is a generator, so onece you try to index it, it's gonna through back error!\n",
    "\n",
    "# doc.sents[0]\n",
    "# it generates the sentences, instead of holding them all into the memory!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "This is the first sentence."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you want to grab the index out of this doc.sents generator you can create an list off of that and that do indexing..\n",
    "\n",
    "list(doc.sents)[0]\n",
    "\n",
    "# there you got, here's our first sentence at index '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets consider some other examples!\n",
    "\n",
    "doc = nlp(u'\"Management is doing the right things; leadership is doing the right things.\" -Peter Ducker')\n",
    "\n",
    "# notice we have the ';' and ',' in the sentence. Let's see how Spacy tries to segment theses sentences!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Management is doing the right things; leadership is doing the right things.\" -Peter Ducker'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first let's check out the text of the doc object!\n",
    "\n",
    "doc.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Management is doing the right things; leadership is doing the right things.\"\n",
      "\n",
      "\n",
      "-Peter Ducker\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sent in doc.sents:\n",
    "    print(sent)\n",
    "    print('\\n')\n",
    "    \n",
    "# it has broken down these both quotes into separate sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Segmentation Rules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's add a NEW RULE to the spacy pipeline\n",
    "\n",
    "# ADD A SEGMENTATION RULE\n",
    "# since we know that every token inside a doc, has it's own index associated. which can be found out by token.i attribute off of the token\n",
    "def test_index(doc):\n",
    "    for token in doc:\n",
    "        print(token.i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n"
     ]
    }
   ],
   "source": [
    "test_index(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's segment the sentences on ';'\n",
    "def set_custom_boundries(doc):\n",
    "    for token in doc.sent[:-1]: # doc.sent[:-1] in order to avoid the index out of range error, we neglect the last word\n",
    "        if token.text == ';':\n",
    "            doc[token.i + 1].is_sent_start = True # the next index word after ';' would be considered as start of the next sentence\n",
    "            \n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp.add_pipe(set_custom_boundries, before = 'parser') # adding our newly created function to nlp pipeline\n",
    "\n",
    "# nlp.pipe_names # making sure our newly built function is added!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc4 = nlp(u'\"Management is doing the right things; leadership is doing the right things.\" -Peter Ducker')\n",
    "\n",
    "for sent in doc4.sents:\n",
    "    print(sent) # Now it separates sentences on ';'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing Segmentation Rules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "mystring = u\"This is a sentence. This is another sentence .\\n\\nThis is a \\nthird sentence.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a sentence. This is another sentence .\n",
      "\n",
      "This is a \n",
      "third sentence.\n"
     ]
    }
   ],
   "source": [
    "print(mystring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a sentence.\n",
      "This is another sentence .\n",
      "\n",
      "\n",
      "This is a \n",
      "third sentence.\n"
     ]
    }
   ],
   "source": [
    "# let's first check the default behaviour\n",
    "\n",
    "doc = nlp(mystring)\n",
    "\n",
    "for sentence in doc.sents:\n",
    "    print(sentence)\n",
    "    \n",
    "# this separates the sentence on periods and white spaces as well apart from \\n\n",
    "# what if we just want to segment a sentence on \\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so let's change default spacy segmentation rules\n",
    "from spacy.pipeline import SentenceSegmenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_newlines(doc): # function to split sentence on a new lines\n",
    "    start = 0\n",
    "    seen_newline = False\n",
    "    \n",
    "    for word in doc:\n",
    "        if seen_newline:\n",
    "            yield doc[start:word.i]\n",
    "            start = word.i\n",
    "            seen_newline = False\n",
    "        \n",
    "        elif word.text.startswith('\\n'):\n",
    "            seen_newline = True\n",
    "            \n",
    "    yield doc[start:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbd = SentenceSegmenter(nlp.vocab, strategy = split_newlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.add_pipe(sbd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(mystring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a sentence. This is another sentence .\n",
      "\n",
      "\n",
      "This is a \n",
      "\n",
      "third sentence.\n"
     ]
    }
   ],
   "source": [
    "for sentence in doc.sents:\n",
    "    print(sentence)\n",
    "    \n",
    "# Now it'll separate each sentence on a new line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
