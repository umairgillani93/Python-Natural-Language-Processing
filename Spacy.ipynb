{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy # importing the library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm') # loading the vocabulary of words!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mystring = '\"We\\'re moving to L.A!\"'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"We\\'re moving to L.A!\"'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mystring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"We're moving to L.A!\"\n"
     ]
    }
   ],
   "source": [
    "print(mystring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\n",
      "We\n",
      "'re\n",
      "moving\n",
      "to\n",
      "L.A\n",
      "!\n",
      "\"\n",
      "['\"', 'We', \"'re\", 'moving', 'to', 'L.A', '!', '\"']\n",
      "\n",
      "\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(mystring) # mystring document \n",
    "\n",
    "token_list = []\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text) # reports back the text of each token in 'doc' string\n",
    "    token_list.append(token.text) \n",
    "    \n",
    "print(token_list)\n",
    "print('\\n')\n",
    "print(len(token_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2 = nlp(u\"We're here to help! send us snail-mail at support@oursite.com, or contact us at http://www.outsite.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We\n",
      "'re\n",
      "here\n",
      "to\n",
      "help\n",
      "!\n",
      "send\n",
      "us\n",
      "snail\n",
      "-\n",
      "mail\n",
      "at\n",
      "support@oursite.com\n",
      ",\n",
      "or\n",
      "contact\n",
      "us\n",
      "at\n",
      "http://www.outsite.com\n"
     ]
    }
   ],
   "source": [
    "for token in doc2:\n",
    "    print(token) # Spacy is smart enough to understand the the punctuations, website links and simple words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n",
      "5\n",
      "km\n",
      "NYC\n",
      "cab\n",
      "ride\n",
      "costs\n",
      "$\n",
      "10.30\n"
     ]
    }
   ],
   "source": [
    "doc3 = nlp(u\"A 5km NYC cab ride costs $10.30\")\n",
    "\n",
    "for token in doc3:\n",
    "    print(token.text) # Here the distance unit and $ sign have assigned their own tokens! and Spacy is smart enough to keep the price together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let\n",
      "'s\n",
      "visit\n",
      "St.\n",
      "Louis\n",
      "in\n",
      "U.S\n",
      "next\n",
      "year\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "doc4 = nlp(u\"Let's visit St. Louis in U.S next year!\")\n",
    "\n",
    "for token in doc4:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57852"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc4.vocab) # reports back the length of 'en_core_web_sm' vocabulary we imported!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc5 = nlp(u\"Apple to built a Hong Kong factory for $6 millions!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple| to| built| a| Hong| Kong| factory| for| $| 6| millions| !| "
     ]
    }
   ],
   "source": [
    "for token in doc5:\n",
    "    print(token.text, end = '| ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple\n",
      "ORG\n",
      "\n",
      "\n",
      "Hong Kong\n",
      "GPE\n",
      "\n",
      "\n",
      "$6 millions\n",
      "MONEY\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for entity in doc5.ents:\n",
    "    print(entity) # reports back the entities in context\n",
    "    print(entity.label_) # off of the entity, it can also extract the labels for each\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple\n",
      "Companies, agencies, institutions, etc.\n",
      "ORG\n",
      "\n",
      "\n",
      "Hong Kong\n",
      "Countries, cities, states\n",
      "GPE\n",
      "\n",
      "\n",
      "$6 millions\n",
      "Monetary values, including unit\n",
      "MONEY\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lets say we want to know, the meanings of each label for entities\n",
    "# Well, we have a simple spacy method for this\n",
    "\n",
    "for entity in doc5.ents:\n",
    "    print(entity)\n",
    "    print(str(spacy.explain(entity.label_)))\n",
    "    print(entity.label_)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noun Chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autonomous cars | insurance liability | the manufacturers | "
     ]
    }
   ],
   "source": [
    "# with Spacy we can also easily grab the Noun chunks from our text\n",
    "# Noun chunk: A word associated with a noun, essentially more highlighting it's meanings\n",
    "\n",
    "# e.g Autonomous Cars, Here, Cars is a noun which is associated with the word 'Autonomous'\n",
    "\n",
    "doc6 = nlp(u\"Autonomous cars shift insurance liability toward the manufacturers\")\n",
    "\n",
    "for chunk in doc6.noun_chunks:\n",
    "    print(chunk, end = ' | ')\n",
    "    # report back the noun chunks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
